{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Discovery and Analysis\n",
    "\n",
    "This notebook performs comprehensive data discovery and analysis on the S3 data lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Data Discovery Notebook Initialized\n",
      "🔗 S3 Ingestion Path: s3://vendor-data-s3/LSEG/TRTH/LSE/ingestion\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from utils.database import DuckDBManager\n",
    "from utils.data_processing import DataQualityChecker\n",
    "from config.settings import config\n",
    "\n",
    "# Initialize components\n",
    "config.setup_logging()\n",
    "db_manager = DuckDBManager()\n",
    "quality_checker = DataQualityChecker()\n",
    "\n",
    "print(\"📊 Data Discovery Notebook Initialized\")\n",
    "print(f\"🔗 S3 Ingestion Path: {config.INGESTION_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Inventory and File Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 17:11:41,086 - utils.database - INFO - S3 credentials configured\n",
      "2025-06-18 17:11:41,087 - utils.database - INFO - DuckDB connection configured successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Discovering data files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0bdf0f479b42d4bd997942b96e477a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 17:16:40,841 - utils.database - INFO - Query executed successfully. Returned 1 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 1 data files\n",
      "📊 Estimated total rows: 5,041,105,550\n",
      "\n",
      "📋 Largest files:\n",
      "                                            filename  estimated_rows  \\\n",
      "0  s3://vendor-data-s3/LSEG/TRTH/LSE/ingestion/20...       100822111   \n",
      "\n",
      "                             file_basename date_from_filename  \\\n",
      "0  LSE-2023-09-01-NORMALIZEDMP-Data-1-of-1                      \n",
      "\n",
      "   total_estimated_rows  \n",
      "0            5041105550  \n"
     ]
    }
   ],
   "source": [
    "# Discover and catalog all data files\n",
    "print(\"🔍 Discovering data files...\")\n",
    "\n",
    "inventory_query = f\"\"\"\n",
    "WITH file_inventory AS (\n",
    "    SELECT \n",
    "        filename,\n",
    "        COUNT(*) as estimated_rows,\n",
    "        REGEXP_EXTRACT(filename, '([^/]+)\\\\.csv\\\\.gz$', 1) as file_basename,\n",
    "        REGEXP_EXTRACT(filename, '(\\\\\\\\d{{4}}-\\\\\\\\d{{2}}-\\\\\\\\d{{2}})', 1) as date_from_filename\n",
    "    FROM read_csv('{config.INGESTION_PATH}/*/*.csv.gz', \n",
    "                 AUTO_DETECT=true, \n",
    "                 FILENAME=true,\n",
    "                 SAMPLE_SIZE=10000) \n",
    "    GROUP BY filename\n",
    ")\n",
    "SELECT \n",
    "    filename,\n",
    "    estimated_rows,\n",
    "    file_basename,\n",
    "    date_from_filename,\n",
    "    estimated_rows * 50 as total_estimated_rows\n",
    "FROM file_inventory\n",
    "ORDER BY estimated_rows DESC\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    inventory = db_manager.execute_query(inventory_query)\n",
    "    \n",
    "    print(f\"✅ Found {len(inventory)} data files\")\n",
    "    print(f\"📊 Estimated total rows: {inventory['total_estimated_rows'].sum():,}\")\n",
    "    \n",
    "    # Display top files by size\n",
    "    print(\"\\n📋 Largest files:\")\n",
    "    print(inventory.head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ File discovery failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing data schema...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 17:16:41,887 - utils.database - INFO - Query executed successfully. Returned 1000 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schema analysis completed\n",
      "📊 Dataset shape: (1000, 114)\n",
      "\n",
      "🔍 Sample Data (first 5 rows):\n",
      "        #RIC        Domain                        Date-Time GMT Offset   Type  \\\n",
      "0  .TRX50GBP  Market Price 2023-09-01 02:59:58.447680-04:00         +1  Trade   \n",
      "1  .TRX50GBP  Market Price 2023-09-01 03:00:58.438312-04:00         +1  Trade   \n",
      "2  .TRX50GBP  Market Price 2023-09-01 03:01:58.408014-04:00         +1  Trade   \n",
      "3  .TRX50GBP  Market Price 2023-09-01 03:02:58.438451-04:00         +1  Trade   \n",
      "4  .TRX50GBP  Market Price 2023-09-01 03:03:58.432486-04:00         +1  Trade   \n",
      "\n",
      "  Ex/Cntrb.ID   LOC   Price Volume Market VWAP  ... Imbalance Activity Type  \\\n",
      "0        None  None  113.90   None        None  ...                    None   \n",
      "1        None  None  114.23   None        None  ...                    None   \n",
      "2        None  None  114.20   None        None  ...                    None   \n",
      "3        None  None  114.17   None        None  ...                    None   \n",
      "4        None  None  114.15   None        None  ...                    None   \n",
      "\n",
      "  Imbalance Side Imbalance Variation Indicator Implied Yield Delta Gamma  \\\n",
      "0           None                          None          None  None  None   \n",
      "1           None                          None          None  None  None   \n",
      "2           None                          None          None  None  None   \n",
      "3           None                          None          None  None  None   \n",
      "4           None                          None          None  None  None   \n",
      "\n",
      "    Rho Theta  Vega                                           filename  \n",
      "0  None  None  None  s3://vendor-data-s3/LSEG/TRTH/LSE/ingestion/20...  \n",
      "1  None  None  None  s3://vendor-data-s3/LSEG/TRTH/LSE/ingestion/20...  \n",
      "2  None  None  None  s3://vendor-data-s3/LSEG/TRTH/LSE/ingestion/20...  \n",
      "3  None  None  None  s3://vendor-data-s3/LSEG/TRTH/LSE/ingestion/20...  \n",
      "4  None  None  None  s3://vendor-data-s3/LSEG/TRTH/LSE/ingestion/20...  \n",
      "\n",
      "[5 rows x 114 columns]\n",
      "\n",
      "📋 Columns (114):\n",
      "  1. #RIC (object)\n",
      "  2. Domain (object)\n",
      "  3. Date-Time (datetime64[us, America/New_York])\n",
      "  4. GMT Offset (object)\n",
      "  5. Type (object)\n",
      "  6. Ex/Cntrb.ID (object)\n",
      "  7. LOC (object)\n",
      "  8. Price (float64)\n",
      "  9. Volume (object)\n",
      "  10. Market VWAP (object)\n",
      "  ... and 104 more columns\n"
     ]
    }
   ],
   "source": [
    "# Analyze schema and data structure\n",
    "print(\"🔍 Analyzing data schema...\")\n",
    "\n",
    "sample_query = f\"\"\"\n",
    "SELECT * \n",
    "FROM read_csv('{config.INGESTION_PATH}/*/*.csv.gz', \n",
    "             AUTO_DETECT=true, \n",
    "             FILENAME=true,\n",
    "             SAMPLE_SIZE=5000)\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    sample_data = db_manager.execute_query(sample_query)\n",
    "    \n",
    "    print(f\"✅ Schema analysis completed\")\n",
    "    print(f\"📊 Dataset shape: {sample_data.shape}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(f\"\\n🔍 Sample Data (first 5 rows):\")\n",
    "    print(sample_data.head())\n",
    "    \n",
    "    # Display column info\n",
    "    print(f\"\\n📋 Columns ({len(sample_data.columns)}):\")\n",
    "    for i, col in enumerate(sample_data.columns[:10]):\n",
    "        print(f\"  {i+1}. {col} ({sample_data[col].dtype})\")\n",
    "    if len(sample_data.columns) > 10:\n",
    "        print(f\"  ... and {len(sample_data.columns) - 10} more columns\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Schema analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Performing data quality assessment...\n",
      "\n",
      "📊 Data Quality Report:\n",
      "Total Rows: 1,000\n",
      "Total Columns: 114\n",
      "\n",
      "❌ Columns with null values (103):\n",
      "  Ex/Cntrb.ID: 1000 nulls (100.0%)\n",
      "  LOC: 1000 nulls (100.0%)\n",
      "  Volume: 1000 nulls (100.0%)\n",
      "  Market VWAP: 1000 nulls (100.0%)\n",
      "  Buyer ID: 1000 nulls (100.0%)\n",
      "  Bid Price: 1000 nulls (100.0%)\n",
      "  Bid Size: 1000 nulls (100.0%)\n",
      "  No. Buyers: 1000 nulls (100.0%)\n",
      "  Seller ID: 1000 nulls (100.0%)\n",
      "  Ask Price: 1000 nulls (100.0%)\n",
      "\n",
      "🔄 Duplicate rows: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Basic data quality assessment\n",
    "print(\"🔎 Performing data quality assessment...\")\n",
    "\n",
    "if 'sample_data' in locals() and not sample_data.empty:\n",
    "    print(\"\\n📊 Data Quality Report:\")\n",
    "    print(f\"Total Rows: {len(sample_data):,}\")\n",
    "    print(f\"Total Columns: {len(sample_data.columns)}\")\n",
    "    \n",
    "    # Check for null values\n",
    "    null_counts = sample_data.isnull().sum()\n",
    "    null_cols = null_counts[null_counts > 0]\n",
    "    \n",
    "    if len(null_cols) > 0:\n",
    "        print(f\"\\n❌ Columns with null values ({len(null_cols)}):\")\n",
    "        for col, count in null_cols.head(10).items():\n",
    "            pct = (count / len(sample_data)) * 100\n",
    "            print(f\"  {col}: {count} nulls ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\n✅ No null values found in sample\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = sample_data.duplicated().sum()\n",
    "    print(f\"\\n🔄 Duplicate rows: {duplicate_count} ({(duplicate_count/len(sample_data)*100):.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No sample data available for quality assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Data Discovery Summary\n",
      "==================================================\n",
      "📁 Total Files: 1\n",
      "📊 Estimated Total Rows: 5,041,105,550\n",
      "📅 Date Range:  to \n",
      "🏗️  Schema: 114 columns\n",
      "📋 Sample Size: 1000 rows\n",
      "📊 Data Types: 109 object, 4 float64, 1 datetime64[us, America/New_York]\n",
      "\n",
      "💡 Recommendations:\n",
      "1. Proceed to bronze layer creation (03_bronze_layer.ipynb)\n",
      "2. Implement data quality monitoring\n",
      "3. Consider partitioning by date for better performance\n",
      "4. Convert to Parquet format for optimization\n",
      "\n",
      "✅ Data discovery completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate final summary and recommendations\n",
    "print(\"📋 Data Discovery Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'inventory' in locals() and not inventory.empty:\n",
    "    print(f\"📁 Total Files: {len(inventory)}\")\n",
    "    print(f\"📊 Estimated Total Rows: {inventory['total_estimated_rows'].sum():,}\")\n",
    "    if 'date_from_filename' in inventory.columns:\n",
    "        valid_dates = inventory['date_from_filename'].dropna()\n",
    "        if len(valid_dates) > 0:\n",
    "            print(f\"📅 Date Range: {valid_dates.min()} to {valid_dates.max()}\")\n",
    "\n",
    "if 'sample_data' in locals() and not sample_data.empty:\n",
    "    print(f\"🏗️  Schema: {sample_data.shape[1]} columns\")\n",
    "    print(f\"📋 Sample Size: {len(sample_data)} rows\")\n",
    "    \n",
    "    # Data type summary\n",
    "    dtype_counts = sample_data.dtypes.value_counts()\n",
    "    print(f\"📊 Data Types: {', '.join([f'{count} {dtype}' for dtype, count in dtype_counts.items()])}\")\n",
    "\n",
    "print(\"\\n💡 Recommendations:\")\n",
    "print(\"1. Proceed to bronze layer creation (03_bronze_layer.ipynb)\")\n",
    "print(\"2. Implement data quality monitoring\")\n",
    "print(\"3. Consider partitioning by date for better performance\")\n",
    "print(\"4. Convert to Parquet format for optimization\")\n",
    "\n",
    "print(\"\\n✅ Data discovery completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
