{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Discovery and Analysis\n",
    "\n",
    "This notebook performs comprehensive data discovery and analysis on the S3 data lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from utils.database import DuckDBManager\n",
    "from utils.data_processing import DataQualityChecker\n",
    "from config.settings import config\n",
    "\n",
    "# Initialize components\n",
    "config.setup_logging()\n",
    "db_manager = DuckDBManager()\n",
    "quality_checker = DataQualityChecker()\n",
    "\n",
    "print(\"\ud83d\udcca Data Discovery Notebook Initialized\")\n",
    "print(f\"\ud83d\udd17 S3 Ingestion Path: {config.INGESTION_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Inventory and File Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover and catalog all data files\n",
    "print(\"\ud83d\udd0d Discovering data files...\")\n",
    "\n",
    "inventory_query = f\"\"\"\n",
    "WITH file_inventory AS (\n",
    "    SELECT \n",
    "        filename,\n",
    "        COUNT(*) as estimated_rows,\n",
    "        REGEXP_EXTRACT(filename, '([^/]+)\\\\.csv\\\\.gz$', 1) as file_basename,\n",
    "        REGEXP_EXTRACT(filename, '(\\\\\\\\d{{4}}-\\\\\\\\d{{2}}-\\\\\\\\d{{2}})', 1) as date_from_filename\n",
    "    FROM read_csv('{config.INGESTION_PATH}/*/*.csv.gz', \n",
    "                 AUTO_DETECT=true, \n",
    "                 FILENAME=true,\n",
    "                 SAMPLE_SIZE=10000) \n",
    "    GROUP BY filename\n",
    ")\n",
    "SELECT \n",
    "    filename,\n",
    "    estimated_rows,\n",
    "    file_basename,\n",
    "    date_from_filename,\n",
    "    estimated_rows * 50 as total_estimated_rows\n",
    "FROM file_inventory\n",
    "ORDER BY estimated_rows DESC\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    inventory = db_manager.execute_query(inventory_query)\n",
    "    \n",
    "    print(f\"\u2705 Found {len(inventory)} data files\")\n",
    "    print(f\"\ud83d\udcca Estimated total rows: {inventory['total_estimated_rows'].sum():,}\")\n",
    "    \n",
    "    # Display top files by size\n",
    "    print(\"\\n\ud83d\udccb Largest files:\")\n",
    "    print(inventory.head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c File discovery failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze schema and data structure\n",
    "print(\"\ud83d\udd0d Analyzing data schema...\")\n",
    "\n",
    "sample_query = f\"\"\"\n",
    "SELECT * \n",
    "FROM read_csv('{config.INGESTION_PATH}/*/*.csv.gz', \n",
    "             AUTO_DETECT=true, \n",
    "             FILENAME=true,\n",
    "             SAMPLE_SIZE=5000)\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    sample_data = db_manager.execute_query(sample_query)\n",
    "    \n",
    "    print(f\"\u2705 Schema analysis completed\")\n",
    "    print(f\"\ud83d\udcca Dataset shape: {sample_data.shape}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(f\"\\n\ud83d\udd0d Sample Data (first 5 rows):\")\n",
    "    print(sample_data.head())\n",
    "    \n",
    "    # Display column info\n",
    "    print(f\"\\n\ud83d\udccb Columns ({len(sample_data.columns)}):\")\n",
    "    for i, col in enumerate(sample_data.columns[:10]):\n",
    "        print(f\"  {i+1}. {col} ({sample_data[col].dtype})\")\n",
    "    if len(sample_data.columns) > 10:\n",
    "        print(f\"  ... and {len(sample_data.columns) - 10} more columns\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Schema analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data quality assessment\n",
    "print(\"\ud83d\udd0e Performing data quality assessment...\")\n",
    "\n",
    "if 'sample_data' in locals() and not sample_data.empty:\n",
    "    print(\"\\n\ud83d\udcca Data Quality Report:\")\n",
    "    print(f\"Total Rows: {len(sample_data):,}\")\n",
    "    print(f\"Total Columns: {len(sample_data.columns)}\")\n",
    "    \n",
    "    # Check for null values\n",
    "    null_counts = sample_data.isnull().sum()\n",
    "    null_cols = null_counts[null_counts > 0]\n",
    "    \n",
    "    if len(null_cols) > 0:\n",
    "        print(f\"\\n\u274c Columns with null values ({len(null_cols)}):\")\n",
    "        for col, count in null_cols.head(10).items():\n",
    "            pct = (count / len(sample_data)) * 100\n",
    "            print(f\"  {col}: {count} nulls ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\n\u2705 No null values found in sample\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = sample_data.duplicated().sum()\n",
    "    print(f\"\\n\ud83d\udd04 Duplicate rows: {duplicate_count} ({(duplicate_count/len(sample_data)*100):.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"\u274c No sample data available for quality assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary and recommendations\n",
    "print(\"\ud83d\udccb Data Discovery Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'inventory' in locals() and not inventory.empty:\n",
    "    print(f\"\ud83d\udcc1 Total Files: {len(inventory)}\")\n",
    "    print(f\"\ud83d\udcca Estimated Total Rows: {inventory['total_estimated_rows'].sum():,}\")\n",
    "    if 'date_from_filename' in inventory.columns:\n",
    "        valid_dates = inventory['date_from_filename'].dropna()\n",
    "        if len(valid_dates) > 0:\n",
    "            print(f\"\ud83d\udcc5 Date Range: {valid_dates.min()} to {valid_dates.max()}\")\n",
    "\n",
    "if 'sample_data' in locals() and not sample_data.empty:\n",
    "    print(f\"\ud83c\udfd7\ufe0f  Schema: {sample_data.shape[1]} columns\")\n",
    "    print(f\"\ud83d\udccb Sample Size: {len(sample_data)} rows\")\n",
    "    \n",
    "    # Data type summary\n",
    "    dtype_counts = sample_data.dtypes.value_counts()\n",
    "    print(f\"\ud83d\udcca Data Types: {', '.join([f'{count} {dtype}' for dtype, count in dtype_counts.items()])}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Recommendations:\")\n",
    "print(\"1. Proceed to bronze layer creation (03_bronze_layer.ipynb)\")\n",
    "print(\"2. Implement data quality monitoring\")\n",
    "print(\"3. Consider partitioning by date for better performance\")\n",
    "print(\"4. Convert to Parquet format for optimization\")\n",
    "\n",
    "print(\"\\n\u2705 Data discovery completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}