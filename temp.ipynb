{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72016f4d-9e4d-4dbb-98bd-f666cd59970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import logging\n",
    "from typing import Optional, Dict, Any, List\n",
    "from pathlib import Path\n",
    "import os\n",
    "from config.settings import config\n",
    "\n",
    "class DuckDBManager:\n",
    "    \"\"\"Manager class for DuckDB operations with S3 integration\"\"\"\n",
    "    \n",
    "    def __init__(self, database_path: Optional[str] = None):\n",
    "        self.database_path = database_path or config.DUCKDB_DATABASE_PATH\n",
    "        self.connection = None\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Ensure database directory exists\n",
    "        os.makedirs(os.path.dirname(self.database_path), exist_ok=True)\n",
    "        \n",
    "    def connect(self) -> duckdb.DuckDBPyConnection:\n",
    "        \"\"\"Create and configure DuckDB connection\"\"\"\n",
    "        if self.connection is None:\n",
    "            self.connection = duckdb.connect(self.database_path)\n",
    "            self._configure_connection()\n",
    "            \n",
    "        return self.connection\n",
    "    \n",
    "    def _configure_connection(self):\n",
    "        \"\"\"Configure DuckDB connection with S3 and performance settings\"\"\"\n",
    "        conn = self.connection\n",
    "        \n",
    "        # Install and load required extensions\n",
    "        conn.execute(\"INSTALL httpfs\")\n",
    "        conn.execute(\"INSTALL aws\") \n",
    "        conn.execute(\"LOAD httpfs\")\n",
    "        conn.execute(\"LOAD aws\")\n",
    "        \n",
    "        # Configure memory and performance settings\n",
    "        conn.execute(f\"SET memory_limit='{config.DUCKDB_MEMORY_LIMIT}'\")\n",
    "        conn.execute(f\"SET threads={config.DUCKDB_THREADS}\")\n",
    "        conn.execute(\"SET enable_progress_bar=true\")\n",
    "        \n",
    "        # Configure S3 credentials\n",
    "        self._configure_s3_credentials(conn)\n",
    "        \n",
    "        self.logger.info(\"DuckDB connection configured successfully\")\n",
    "    \n",
    "    def _configure_s3_credentials(self, conn):\n",
    "        \"\"\"Configure S3 credentials for DuckDB\"\"\"\n",
    "        if config.AWS_PROFILE:\n",
    "            # Use AWS profile\n",
    "            conn.execute(f\"\"\"\n",
    "                CREATE OR REPLACE SECRET s3_secret (\n",
    "                    TYPE s3,\n",
    "                    PROVIDER credential_chain,\n",
    "                    CHAIN 'config',\n",
    "                    PROFILE '{config.AWS_PROFILE}'\n",
    "                )\n",
    "            \"\"\")\n",
    "        elif config.AWS_ACCESS_KEY_ID and config.AWS_SECRET_ACCESS_KEY:\n",
    "            # Use explicit credentials\n",
    "            conn.execute(f\"\"\"\n",
    "                CREATE OR REPLACE SECRET s3_secret (\n",
    "                    TYPE s3,\n",
    "                    KEY_ID 'AKIAUHS3R6WXNCISGIMU',\n",
    "                    SECRET '7Ea3JzkTOTEhVEIxsMf6ws0zeVZ13Qk',\n",
    "                    REGION 'us-east-1'\n",
    "                )\n",
    "            \"\"\")\n",
    "        else:\n",
    "            # Use credential chain (environment variables, IAM roles, etc.)\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE OR REPLACE SECRET s3_secret (\n",
    "                    TYPE s3,\n",
    "                    PROVIDER credential_chain\n",
    "                )\n",
    "            \"\"\")\n",
    "        \n",
    "        self.logger.info(\"S3 credentials configured\")\n",
    "    \n",
    "    def test_s3_connection(self) -> bool:\n",
    "        \"\"\"Test S3 connection by listing files\"\"\"\n",
    "        try:\n",
    "            conn = self.connect()\n",
    "            result = conn.execute(f\"\"\"\n",
    "                SELECT filename, 1 as record_count\n",
    "                FROM read_csv('{config.INGESTION_PATH}/*/*.csv.gz', \n",
    "                             AUTO_DETECT=true, \n",
    "                             FILENAME=true,\n",
    "                             SAMPLE_SIZE=100) \n",
    "                GROUP BY filename\n",
    "                LIMIT 5\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            self.logger.info(f\"S3 connection test successful. Found {len(result)} files\")\n",
    "            for file_info in result:\n",
    "                self.logger.info(f\"  File: {file_info[0]}, Records: {file_info[1]}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"S3 connection test failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def execute_query(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Execute SQL query and return pandas DataFrame\"\"\"\n",
    "        try:\n",
    "            conn = self.connect()\n",
    "            result = conn.execute(query).df()\n",
    "            self.logger.info(f\"Query executed successfully. Returned {len(result)} rows\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Query execution failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def execute_sql(self, sql: str) -> Any:\n",
    "        \"\"\"Execute SQL statement (non-query)\"\"\"\n",
    "        try:\n",
    "            conn = self.connect()\n",
    "            result = conn.execute(sql)\n",
    "            self.logger.info(\"SQL statement executed successfully\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"SQL execution failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_table_from_s3(self, table_name: str, s3_path: str, **kwargs) -> bool:\n",
    "        \"\"\"Create table from S3 CSV files\"\"\"\n",
    "        try:\n",
    "            conn = self.connect()\n",
    "            \n",
    "            # Generate CREATE TABLE statement\n",
    "            sql = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE {table_name} AS\n",
    "                SELECT * FROM read_csv('{s3_path}', \n",
    "                                     AUTO_DETECT=true,\n",
    "                                     UNION_BY_NAME=true,\n",
    "                                     FILENAME=true,\n",
    "                                     {', '.join([f\"{k}={v}\" for k, v in kwargs.items()])})\n",
    "            \"\"\"\n",
    "            \n",
    "            conn.execute(sql)\n",
    "            self.logger.info(f\"Table {table_name} created from {s3_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create table {table_name}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def export_to_s3(self, table_name: str, s3_path: str, \n",
    "                     format: str = 'parquet', **kwargs) -> bool:\n",
    "        \"\"\"Export table to S3\"\"\"\n",
    "        try:\n",
    "            conn = self.connect()\n",
    "            \n",
    "            copy_options = ', '.join([f\"{k} {v}\" for k, v in kwargs.items()])\n",
    "            if copy_options:\n",
    "                copy_options = f\"({copy_options})\"\n",
    "            \n",
    "            sql = f\"\"\"\n",
    "                COPY {table_name} TO '{s3_path}' \n",
    "                (FORMAT {format} {copy_options})\n",
    "            \"\"\"\n",
    "            \n",
    "            conn.execute(sql)\n",
    "            self.logger.info(f\"Table {table_name} exported to {s3_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to export table {table_name}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_table_info(self, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Get table schema information\"\"\"\n",
    "        return self.execute_query(f\"DESCRIBE {table_name}\")\n",
    "    \n",
    "    def list_tables(self) -> pd.DataFrame:\n",
    "        \"\"\"List all tables in the database\"\"\"\n",
    "        return self.execute_query(\"SHOW TABLES\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            self.connection = None\n",
    "            self.logger.info(\"Database connection closed\")\n",
    "\n",
    "class S3Manager:\n",
    "    \"\"\"Manager class for direct S3 operations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.s3_client = self._create_s3_client()\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def _create_s3_client(self):\n",
    "        \"\"\"Create S3 client with appropriate credentials\"\"\"\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id=config.AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=config.AWS_SECRET_ACCESS_KEY,\n",
    "            region_name=config.AWS_DEFAULT_REGION,\n",
    "            profile_name=config.AWS_PROFILE\n",
    "        )\n",
    "        \n",
    "        return session.client('s3', endpoint_url=config.S3_ENDPOINT_URL)\n",
    "    \n",
    "    def list_files(self, prefix: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"List files in S3 bucket with given prefix\"\"\"\n",
    "        try:\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=config.S3_BUCKET,\n",
    "                Prefix=prefix\n",
    "            )\n",
    "            \n",
    "            files = []\n",
    "            for obj in response.get('Contents', []):\n",
    "                files.append({\n",
    "                    'key': obj['Key'],\n",
    "                    'size': obj['Size'],\n",
    "                    'last_modified': obj['LastModified'],\n",
    "                    'etag': obj['ETag']\n",
    "                })\n",
    "            \n",
    "            return files\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to list files with prefix {prefix}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_file_info(self, key: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get metadata for a specific S3 object\"\"\"\n",
    "        try:\n",
    "            response = self.s3_client.head_object(\n",
    "                Bucket=config.S3_BUCKET,\n",
    "                Key=key\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'size': response['ContentLength'],\n",
    "                'last_modified': response['LastModified'],\n",
    "                'content_type': response.get('ContentType'),\n",
    "                'etag': response['ETag']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to get file info for {key}: {e}\")\n",
    "            return None\n",
    "\n",
    "# Create global instances\n",
    "db_manager = DuckDBManager()\n",
    "s3_manager = S3Manager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d21cf2-0d50-4828-b213-29d75eb63367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:40:49,886 - config.settings - INFO - Starting Data Lake Setup\n"
     ]
    }
   ],
   "source": [
    "logger = config.setup_logging()\n",
    "logger.info(\"Starting Data Lake Setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314626d1-4ef0-4553-b3be-ba06955fe5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration validation passed\n",
      "\n",
      "üìã Configuration Summary:\n",
      "S3 Bucket: vendor-data-s3\n",
      "AWS Region: us-east-1\n",
      "DuckDB Database: ./data_lake.duckdb\n",
      "Memory Limit: 8GB\n",
      "Threads: 4\n"
     ]
    }
   ],
   "source": [
    "# Validate configuration\n",
    "try:\n",
    "    config.validate_config()\n",
    "    print(\"‚úÖ Configuration validation passed\")\n",
    "    \n",
    "    # Display key configuration settings\n",
    "    print(\"\\nüìã Configuration Summary:\")\n",
    "    print(f\"S3 Bucket: {config.S3_BUCKET}\")\n",
    "    print(f\"AWS Region: {config.AWS_DEFAULT_REGION}\")\n",
    "    print(f\"DuckDB Database: {config.DUCKDB_DATABASE_PATH}\")\n",
    "    print(f\"Memory Limit: {config.DUCKDB_MEMORY_LIMIT}\")\n",
    "    print(f\"Threads: {config.DUCKDB_THREADS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration validation failed: {e}\")\n",
    "    print(\"Please check your .env file and ensure all required variables are set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c544edc0-ad8c-4516-a977-5057058556ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:41:15,525 - __main__ - INFO - S3 credentials configured\n",
      "2025-06-18 14:41:15,526 - __main__ - INFO - DuckDB connection configured successfully\n",
      "2025-06-18 14:41:15,537 - __main__ - INFO - Query executed successfully. Returned 1 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DuckDB connection established\n",
      "Test query result: Hello DuckDB!\n"
     ]
    }
   ],
   "source": [
    "# Initialize DuckDB connection\n",
    "try:\n",
    "    conn = db_manager.connect()\n",
    "    print(\"‚úÖ DuckDB connection established\")\n",
    "    \n",
    "    # Test basic functionality\n",
    "    test_result = db_manager.execute_query(\"SELECT 'Hello DuckDB!' as message\")\n",
    "    print(f\"Test query result: {test_result['message'].iloc[0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to connect to DuckDB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2453db4-1e51-4e97-8424-275651acbaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:41:38,665 - __main__ - ERROR - S3 connection test failed: HTTP Error: HTTP GET error reading 's3://vendor-data-s3/LSEG/TRTH/LSE/ingestion/?encoding-type=url&list-type=2&prefix=LSEG%2FTRTH%2FLSE%2Fingestion%2F' in region 'us-east-1' (HTTP 403 Forbidden)\n",
      "\n",
      "Authentication Failure - this is usually caused by invalid or missing credentials.\n",
      "* Credentials are provided, but they did not work.\n",
      "* See https://duckdb.org/docs/stable/extensions/httpfs/s3api.html\n"
     ]
    }
   ],
   "source": [
    "s3_test = db_manager.test_s3_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a0a73-b1ee-4309-b889-fba8cdf47d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
