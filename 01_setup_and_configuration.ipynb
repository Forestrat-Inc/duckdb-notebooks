{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Lake Setup and Configuration\n",
    "\n",
    "This notebook sets up the DuckDB data lake with S3 integration and performs initial configuration.\n",
    "\n",
    "## Prerequisites\n",
    "1. Install required packages: `pip install -r requirements.txt`\n",
    "2. Copy `.env.example` to `.env` and configure your AWS credentials\n",
    "3. Ensure you have access to the S3 bucket `vendor-data-s3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:05:24,965 - config.settings - INFO - Starting Data Lake Setup\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import custom modules\n",
    "from config import config\n",
    "from utils import db_manager, s3_manager\n",
    "\n",
    "# Setup logging\n",
    "logger = config.setup_logging()\n",
    "logger.info(\"Starting Data Lake Setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Validate Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration validation passed\n",
      "\n",
      "üìã Configuration Summary:\n",
      "S3 Bucket: vendor-data-s3\n",
      "AWS Region: us-east-1\n",
      "DuckDB Database: ./data_lake.duckdb\n",
      "Memory Limit: 8GB\n",
      "Threads: 4\n"
     ]
    }
   ],
   "source": [
    "# Validate configuration\n",
    "try:\n",
    "    config.validate_config()\n",
    "    print(\"‚úÖ Configuration validation passed\")\n",
    "    \n",
    "    # Display key configuration settings\n",
    "    print(\"\\nüìã Configuration Summary:\")\n",
    "    print(f\"S3 Bucket: {config.S3_BUCKET}\")\n",
    "    print(f\"AWS Region: {config.AWS_DEFAULT_REGION}\")\n",
    "    print(f\"DuckDB Database: {config.DUCKDB_DATABASE_PATH}\")\n",
    "    print(f\"Memory Limit: {config.DUCKDB_MEMORY_LIMIT}\")\n",
    "    print(f\"Threads: {config.DUCKDB_THREADS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration validation failed: {e}\")\n",
    "    print(\"Please check your .env file and ensure all required variables are set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize DuckDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:05:29,829 - utils.database - INFO - S3 credentials configured\n",
      "2025-06-18 16:05:29,830 - utils.database - INFO - DuckDB connection configured successfully\n",
      "2025-06-18 16:05:29,832 - utils.database - INFO - Query executed successfully. Returned 1 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DuckDB connection established\n",
      "Test query result: Hello DuckDB!\n"
     ]
    }
   ],
   "source": [
    "# Initialize DuckDB connection\n",
    "try:\n",
    "    conn = db_manager.connect()\n",
    "    print(\"‚úÖ DuckDB connection established\")\n",
    "    \n",
    "    # Test basic functionality\n",
    "    test_result = db_manager.execute_query(\"SELECT 'Hello DuckDB!' as message\")\n",
    "    print(f\"Test query result: {test_result['message'].iloc[0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to connect to DuckDB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test S3 Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Testing S3 connectivity...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41fcd7279374bcb811929a4ac90afee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:10:41,035 - utils.database - INFO - S3 connection test successful. Found 1 files\n",
      "2025-06-18 16:10:41,037 - utils.database - INFO -   File: s3://vendor-data-s3/LSEG/TRTH/LSE/ingestion/20230901/LSE-2023-09-01-NORMALIZEDMP-Data-1-of-1.csv.gz, Records: 100822111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DuckDB S3 integration working\n",
      "‚úÖ Found 5 files in S3 bucket\n",
      "\n",
      "üìÅ Sample files:\n",
      "  LSEG/TRTH/LSE/ingestion/ (0.0 MB)\n",
      "  LSEG/TRTH/LSE/ingestion/2023-09-01/data/merged/LSE-2023-09-01-NORMALIZEDMP-Data-1-of-1.csv.gz (1754.5 MB)\n",
      "  LSEG/TRTH/LSE/ingestion/2023-09-01/data/merged/LSE-2023-09-01-NORMALIZEDMP-Report-1-of-1.csv.gz (1.0 MB)\n",
      "  LSEG/TRTH/LSE/ingestion/20230901/ (0.0 MB)\n",
      "  LSEG/TRTH/LSE/ingestion/20230901/LSE-2023-09-01-NORMALIZEDMP-Data-1-of-1.csv.gz (1754.5 MB)\n"
     ]
    }
   ],
   "source": [
    "# Test S3 connectivity\n",
    "print(\"üîó Testing S3 connectivity...\")\n",
    "\n",
    "try:\n",
    "    # Test DuckDB S3 integration\n",
    "    s3_test = db_manager.test_s3_connection()\n",
    "    \n",
    "    if s3_test:\n",
    "        print(\"‚úÖ DuckDB S3 integration working\")\n",
    "    else:\n",
    "        print(\"‚ùå DuckDB S3 integration failed\")\n",
    "    \n",
    "    # List files in S3 bucket using boto3\n",
    "    files = s3_manager.list_files('LSEG/TRTH/LSE/ingestion/')\n",
    "    \n",
    "    if files:\n",
    "        print(f\"‚úÖ Found {len(files)} files in S3 bucket\")\n",
    "        print(\"\\nüìÅ Sample files:\")\n",
    "        for file in files[:5]:  # Show first 5 files\n",
    "            size_mb = file['size'] / (1024 * 1024)\n",
    "            print(f\"  {file['key']} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"‚ùå No files found in S3 bucket\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå S3 connectivity test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Database Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database schema for data lake layers\n",
    "print(\"üèóÔ∏è Creating database schema...\")\n",
    "\n",
    "schema_sql = \"\"\"\n",
    "-- Create schemas for different data lake layers\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS silver;\n",
    "CREATE SCHEMA IF NOT EXISTS gold;\n",
    "CREATE SCHEMA IF NOT EXISTS staging;\n",
    "CREATE SCHEMA IF NOT EXISTS audit;\n",
    "\n",
    "-- Create audit tables for tracking data lineage\n",
    "CREATE TABLE IF NOT EXISTS audit.data_ingestion_log (\n",
    "    ingestion_id UUID DEFAULT gen_random_uuid(),\n",
    "    source_path VARCHAR,\n",
    "    file_size BIGINT,\n",
    "    record_count BIGINT,\n",
    "    ingestion_timestamp TIMESTAMP DEFAULT NOW(),\n",
    "    data_date DATE,\n",
    "    processing_status VARCHAR DEFAULT 'bronze',\n",
    "    error_message VARCHAR,\n",
    "    created_at TIMESTAMP DEFAULT NOW()\n",
    ");\n",
    "\n",
    "-- Create pipeline state tracking\n",
    "CREATE TABLE IF NOT EXISTS audit.pipeline_state (\n",
    "    pipeline_name VARCHAR PRIMARY KEY,\n",
    "    last_processed_date DATE,\n",
    "    last_processed_file VARCHAR,\n",
    "    processing_status VARCHAR,\n",
    "    last_updated TIMESTAMP DEFAULT NOW(),\n",
    "    error_message VARCHAR\n",
    ");\n",
    "\n",
    "-- Create data quality checks table\n",
    "CREATE TABLE IF NOT EXISTS audit.data_quality_checks (\n",
    "    check_id UUID DEFAULT gen_random_uuid(),\n",
    "    check_name VARCHAR,\n",
    "    table_name VARCHAR,\n",
    "    check_timestamp TIMESTAMP DEFAULT NOW(),\n",
    "    records_checked BIGINT,\n",
    "    records_failed BIGINT,\n",
    "    failure_rate DECIMAL(5,2),\n",
    "    check_status VARCHAR,\n",
    "    error_details VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    db_manager.execute_sql(schema_sql)\n",
    "    print(\"‚úÖ Database schema created successfully\")\n",
    "    \n",
    "    # Verify tables were created\n",
    "    tables = db_manager.list_tables()\n",
    "    print(f\"\\nüìä Created {len(tables)} tables:\")\n",
    "    for _, table in tables.iterrows():\n",
    "        print(f\"  {table['schema']}.{table['name']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create schema: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Pipeline State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline state\n",
    "print(\"üîÑ Initializing pipeline state...\")\n",
    "\n",
    "init_pipeline_sql = \"\"\"\n",
    "INSERT INTO audit.pipeline_state (pipeline_name, last_processed_date, processing_status)\n",
    "VALUES ('lse_market_data', '2023-08-31', 'ready')\n",
    "ON CONFLICT (pipeline_name) DO UPDATE SET\n",
    "    processing_status = 'ready',\n",
    "    last_updated = NOW();\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    db_manager.execute_sql(init_pipeline_sql)\n",
    "    \n",
    "    # Check pipeline state\n",
    "    pipeline_state = db_manager.execute_query(\"SELECT * FROM audit.pipeline_state\")\n",
    "    print(\"‚úÖ Pipeline state initialized\")\n",
    "    print(\"\\nüìä Current pipeline state:\")\n",
    "    print(pipeline_state)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize pipeline state: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Data Access from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading data from S3\n",
    "print(\"üìñ Testing data access from S3...\")\n",
    "\n",
    "test_query = f\"\"\"\n",
    "SELECT \n",
    "    filename,\n",
    "    COUNT(*) as row_count\n",
    "FROM read_csv('{config.INGESTION_PATH}/*/*.csv.gz', \n",
    "             AUTO_DETECT=true, \n",
    "             FILENAME=true,\n",
    "             SAMPLE_SIZE=10000) \n",
    "GROUP BY filename\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    sample_data = db_manager.execute_query(test_query)\n",
    "    \n",
    "    if not sample_data.empty:\n",
    "        print(\"‚úÖ Successfully read data from S3\")\n",
    "        print(f\"\\nüìä Sample data from {len(sample_data)} files:\")\n",
    "        print(sample_data)\n",
    "        \n",
    "        total_rows = sample_data['row_count'].sum()\n",
    "        print(f\"\\nTotal rows in sample: {total_rows:,}\")\n",
    "    else:\n",
    "        print(\"‚ùå No data found in S3 files\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to read data from S3: {e}\")\n",
    "    print(\"Check your S3 credentials and file paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Schema Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover data schema\n",
    "print(\"üîç Discovering data schema...\")\n",
    "\n",
    "schema_query = f\"\"\"\n",
    "DESCRIBE (\n",
    "    SELECT * FROM read_csv('{config.INGESTION_PATH}/*/*.csv.gz', \n",
    "                          AUTO_DETECT=true,\n",
    "                          SAMPLE_SIZE=50000) \n",
    "    LIMIT 0\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    schema_info = db_manager.execute_query(schema_query)\n",
    "    \n",
    "    print(\"‚úÖ Schema discovery completed\")\n",
    "    print(f\"\\nüìä Discovered {len(schema_info)} columns:\")\n",
    "    print(schema_info)\n",
    "    \n",
    "    # Save schema info for later use\n",
    "    schema_info.to_csv('discovered_schema.csv', index=False)\n",
    "    print(\"\\nüíæ Schema saved to 'discovered_schema.csv'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Schema discovery failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Settings Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 16:01:27,902 - utils.database - ERROR - SQL execution failed: Parser Error: Unrecognized print format true, supported formats: [json, query_tree, query_tree_optimizer, no_output]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Optimizing performance settings...\n",
      "‚ùå Failed to optimize settings: Parser Error: Unrecognized print format true, supported formats: [json, query_tree, query_tree_optimizer, no_output]\n"
     ]
    }
   ],
   "source": [
    "# Optimize DuckDB settings for large data processing\n",
    "print(\"‚ö° Optimizing performance settings...\")\n",
    "\n",
    "performance_sql = f\"\"\"\n",
    "-- Set performance parameters\n",
    "SET memory_limit='{config.DUCKDB_MEMORY_LIMIT}';\n",
    "SET threads={config.DUCKDB_THREADS};\n",
    "SET enable_progress_bar=true;\n",
    "SET enable_profiling=true;\n",
    "SET profiling_output='./query_profile.json';\n",
    "\n",
    "-- Set S3 specific optimizations\n",
    "SET s3_region='{config.AWS_DEFAULT_REGION}';\n",
    "SET enable_http_metadata_cache=true;\n",
    "SET http_timeout=30000;\n",
    "\n",
    "-- Optimize for analytical workloads\n",
    "SET default_order='ASC';\n",
    "SET enable_object_cache=true;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    db_manager.execute_sql(performance_sql)\n",
    "    print(\"‚úÖ Performance settings optimized\")\n",
    "    \n",
    "    # Display current settings\n",
    "    settings_query = \"\"\"\n",
    "    SELECT name, value, description \n",
    "    FROM duckdb_settings() \n",
    "    WHERE name IN ('memory_limit', 'threads', 'enable_progress_bar')\n",
    "    \"\"\"\n",
    "    \n",
    "    current_settings = db_manager.execute_query(settings_query)\n",
    "    print(\"\\n‚öôÔ∏è Current settings:\")\n",
    "    print(current_settings)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to optimize settings: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create useful helper functions\n",
    "print(\"üõ†Ô∏è Creating helper functions...\")\n",
    "\n",
    "helper_functions_sql = \"\"\"\n",
    "-- Function to get data freshness\n",
    "CREATE OR REPLACE FUNCTION get_data_freshness(table_name VARCHAR)\n",
    "RETURNS TABLE(\n",
    "    latest_date DATE,\n",
    "    days_ago INTEGER,\n",
    "    record_count BIGINT\n",
    ") AS (\n",
    "    SELECT \n",
    "        MAX(data_date) as latest_date,\n",
    "        CAST(current_date - MAX(data_date) AS INTEGER) as days_ago,\n",
    "        COUNT(*) as record_count\n",
    "    FROM IDENTIFIER(table_name)\n",
    "    WHERE data_date IS NOT NULL\n",
    ");\n",
    "\n",
    "-- Function to get table statistics\n",
    "CREATE OR REPLACE FUNCTION get_table_stats(table_name VARCHAR)\n",
    "RETURNS TABLE(\n",
    "    total_rows BIGINT,\n",
    "    total_columns INTEGER,\n",
    "    null_values BIGINT,\n",
    "    duplicates BIGINT\n",
    ") AS (\n",
    "    WITH stats AS (\n",
    "        SELECT \n",
    "            COUNT(*) as total_rows,\n",
    "            COUNT(*) - COUNT(DISTINCT *) as duplicates\n",
    "        FROM IDENTIFIER(table_name)\n",
    "    )\n",
    "    SELECT \n",
    "        total_rows,\n",
    "        0 as total_columns,  -- Placeholder\n",
    "        0 as null_values,    -- Placeholder\n",
    "        duplicates\n",
    "    FROM stats\n",
    ");\n",
    "\n",
    "-- Function to log processing events\n",
    "CREATE OR REPLACE FUNCTION log_processing_event(\n",
    "    event_type VARCHAR,\n",
    "    table_name VARCHAR,\n",
    "    record_count BIGINT DEFAULT 0,\n",
    "    message VARCHAR DEFAULT NULL\n",
    ")\n",
    "RETURNS VARCHAR AS (\n",
    "    INSERT INTO audit.data_ingestion_log \n",
    "    (source_path, record_count, processing_status, error_message)\n",
    "    VALUES (table_name, record_count, event_type, message);\n",
    "    \n",
    "    RETURN 'Event logged successfully';\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    db_manager.execute_sql(helper_functions_sql)\n",
    "    print(\"‚úÖ Helper functions created successfully\")\n",
    "    \n",
    "    # Test a helper function\n",
    "    test_result = db_manager.execute_query(\n",
    "        \"SELECT log_processing_event('setup', 'data_lake_initialization', 0, 'Setup completed') as result\"\n",
    "    )\n",
    "    print(f\"Function test result: {test_result['result'].iloc[0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create helper functions: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Setup Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setup summary\n",
    "print(\"üìã Data Lake Setup Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Get database info\n",
    "    db_info = db_manager.execute_query(\"SELECT current_database(), version()\")\n",
    "    \n",
    "    # Get table count\n",
    "    tables = db_manager.list_tables()\n",
    "    \n",
    "    # Get pipeline state\n",
    "    pipeline_state = db_manager.execute_query(\"SELECT * FROM audit.pipeline_state\")\n",
    "    \n",
    "    print(f\"‚úÖ DuckDB Version: {db_info.iloc[0]['version()']}\")\n",
    "    print(f\"‚úÖ Database: {db_info.iloc[0]['current_database()']}\")\n",
    "    print(f\"‚úÖ Tables Created: {len(tables)}\")\n",
    "    print(f\"‚úÖ Schemas: bronze, silver, gold, staging, audit\")\n",
    "    print(f\"‚úÖ S3 Integration: Configured\")\n",
    "    print(f\"‚úÖ Pipeline State: Initialized\")\n",
    "    \n",
    "    print(\"\\nüéâ Data lake setup completed successfully!\")\n",
    "    print(\"\\nüìå Next Steps:\")\n",
    "    print(\"1. Run '02_data_discovery.ipynb' to explore your data\")\n",
    "    print(\"2. Run '03_bronze_layer.ipynb' to create the bronze layer\")\n",
    "    print(\"3. Run '04_silver_layer.ipynb' to clean and transform data\")\n",
    "    print(\"4. Run '05_gold_layer.ipynb' to create analytics-ready data\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error generating summary: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Clean up (optional - you might want to keep connection open)\n",
    "    # db_manager.close()\n",
    "    print(\"\\n‚ú® Setup notebook completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
